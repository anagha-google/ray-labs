{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Job API submission of distributed Scikit-Learn on Ray on Vertex AI"
      ],
      "metadata": {
        "id": "obteNyiC7Qr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Local setup to submit job to Ray cluster"
      ],
      "metadata": {
        "id": "XKc7Bc3w2vEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path as path\n",
        "\n",
        "root_path = path.cwd()\n",
        "ray_lab_local_dir = root_path / \"ray_lab_local_dir\"\n",
        "script_path = ray_lab_local_dir / \"code\"\n",
        "script_path.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "mn8b1HlevK3h",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709256420989,
          "user_tz": 360,
          "elapsed": 126,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Create the training script and persist locally"
      ],
      "metadata": {
        "id": "QMngEVZs2zt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_id_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "PROJECT_ID = project_id_output[0]\n",
        "\n",
        "project_nbr_output = !gcloud projects describe $PROJECT_ID --format='value(projectNumber)'\n",
        "PROJECT_NBR = project_nbr_output[0]\n",
        "REGION=\"us-central1\"\n",
        "\n",
        "training_script = \"\"\"\n",
        "import time, sys, joblib\n",
        "import numpy as np, pandas as pd\n",
        "import ray\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer,make_column_transformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix,confusion_matrix,classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from google.cloud import bigquery, aiplatform\n",
        "from google.cloud.aiplatform.preview import vertex_ray\n",
        "from ray.runtime_env import RuntimeEnv\n",
        "from ray.air.config import RunConfig, CheckpointConfig, ScalingConfig\n",
        "from ray.util.joblib import register_ray\n",
        "\n",
        "RAY_ADDRESS=\"vertex_ray://projects/_REPLACE_PROJECT_NBR_/locations/_REPLACE_REGION_/persistentResources/ray-kicking-tires-cluster\"\n",
        "\n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "\n",
        "aiplatform.init(project=\"_REPLACE_PROJECT_ID_\", location=\"_REPLACE_REGION_\")\n",
        "register_ray()\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "# The below statement will parallelize all code placed below it\n",
        "with joblib.parallel_backend('ray'):\n",
        "\n",
        "  # Column listing\n",
        "  numerical_columns_list=[\"culmen_length_mm\",\"culmen_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]\n",
        "  categorical_columns_list=[\"island\", \"sex\"]\n",
        "\n",
        "  # Read training data from BigQuery\n",
        "  client = bigquery.Client(project=\"_REPLACE_PROJECT_ID_\")\n",
        "  source_df = client.query(\"SELECT * FROM `ray_lab_ds.penguins_curated`\").to_dataframe()\n",
        "\n",
        "  # Features\n",
        "  X = source_df.drop(columns = ['species'])\n",
        "\n",
        "  # Label\n",
        "  Y = source_df['species']\n",
        "\n",
        "  # Split into train and test data\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 123)\n",
        "\n",
        "  # Preprocessing of numerical data\n",
        "  numerical_transformer = SimpleImputer(strategy='mean')\n",
        "  numerical_scaler = MinMaxScaler()\n",
        "\n",
        "  # Preprocessing for categorical data\n",
        "  categorical_preprocessing_pipe = Pipeline(steps=[\n",
        "      ('cat_col_imputer', SimpleImputer(strategy='most_frequent')),\n",
        "      ('cat_col_onehotencoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "  ])\n",
        "\n",
        "  # Bundle preprocessing for numerical imputer and categorical preprocessing pipeline\n",
        "  preprocessor = ColumnTransformer(\n",
        "      transformers=[\n",
        "          ('num_col_imputer', numerical_transformer, numerical_columns_list),\n",
        "          ('cat_col_preprocessor', categorical_preprocessing_pipe, categorical_columns_list)\n",
        "      ])\n",
        "\n",
        "  random_forest_model = RandomForestClassifier(n_estimators=10)\n",
        "\n",
        "  # Bundle preprocessing and modeling code in a pipeline\n",
        "  penguin_training_pipeline = Pipeline(steps=[\n",
        "      ('preprocessor', preprocessor),\n",
        "      ('scaler', numerical_scaler),\n",
        "      ('model', random_forest_model)])\n",
        "\n",
        "  penguin_training_pipeline.fit(X_train, Y_train)\n",
        "\n",
        "  # Testing\n",
        "  penguin_predictions = penguin_training_pipeline.predict(X_test)\n",
        "  penguin_predictions\n",
        "\n",
        "  print('Accuracy : ', accuracy_score(Y_test, penguin_predictions))\n",
        "  print('F1 Score : ', f1_score(Y_test, penguin_predictions, average = 'weighted'))\n",
        "  print('Precision : ', precision_score(Y_test, penguin_predictions , average = 'weighted'))\n",
        "  print('Recall : ', recall_score(Y_test, penguin_predictions, average = 'weighted'))\n",
        "\"\"\"\n",
        "\n",
        "training_script=training_script.replace(\"_REPLACE_PROJECT_ID_\",PROJECT_ID)\n",
        "training_script=training_script.replace(\"_REPLACE_PROJECT_NBR_\",PROJECT_NBR)\n",
        "training_script=training_script.replace(\"_REPLACE_REGION_\", REGION)\n",
        "\n",
        "with open(script_path / \"task.py\", \"w\") as f:\n",
        "    f.write(training_script)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "EAunzkutuZH0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709256423896,
          "user_tz": 360,
          "elapsed": 2756,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Create the requirements script and persist locally"
      ],
      "metadata": {
        "id": "0Ovp-WDT26mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "requirements = \"\"\"\n",
        "google-cloud-aiplatform[ray]==1.40.0\n",
        "ray[data]==2.4.0\n",
        "ray[train]==2.4.0\n",
        "ray[tune]==2.4.0\n",
        "scikit-learn==1.2.2\n",
        "google-cloud-bigquery\n",
        "google-cloud-aiplatform\n",
        "joblib\n",
        "pandas<2.0.0\n",
        "db-dtypes\n",
        "\"\"\"\n",
        "\n",
        "with open(ray_lab_local_dir / \"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "LVQzwBk1w_e_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709256424064,
          "user_tz": 360,
          "elapsed": 169,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [4] Submit script to Ray cluster using jobs API & poll for completion"
      ],
      "metadata": {
        "id": "fCv5lcFM3CzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform as vertex_ai\n",
        "import random, string, time, ray\n",
        "from ray.job_submission import JobSubmissionClient, JobStatus\n",
        "import vertex_ray\n",
        "from vertex_ray import Resources\n",
        "from datetime import datetime\n",
        "\n",
        "project_id_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "PROJECT_ID = project_id_output[0]\n",
        "project_nbr_output = !gcloud projects describe $PROJECT_ID --format='value(projectNumber)'\n",
        "PROJECT_NBR = project_nbr_output[0]\n",
        "REGION=\"us-central1\"\n",
        "CLUSTER_NAME=\"ray-kicking-tires-cluster\"\n",
        "AIP_BUCKET_URI = f\"gs://ray_lab_log_bucket_{PROJECT_NBR}/\"\n",
        "RAY_CLUSTER_RESOURCE_NAME='projects/{}/locations/{}/persistentResources/{}'.format(PROJECT_NBR,REGION,CLUSTER_NAME)\n",
        "\n",
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=AIP_BUCKET_URI)\n",
        "ray_client = JobSubmissionClient(\"vertex_ray://{}\".format(RAY_CLUSTER_RESOURCE_NAME))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p84HU0Smzdqd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709256428424,
          "user_tz": 360,
          "elapsed": 4362,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ef0d0065-61c4-402c-b122-65e2d22df5f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ray on Vertex AI]: Cluster State = State.RUNNING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_NAME = \"penguin_species_predictor\"\n",
        "RAY_JOB_LOGGING_URI = f\"gs://ray_lab_log_bucket_{PROJECT_NBR}/logs\"\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "job_id_suffix = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
        "\n",
        "job_id = ray_client.submit_job(\n",
        "    submission_id=f\"penguin-trainer-{TIMESTAMP}-{job_id_suffix}\",\n",
        "    entrypoint=f\"python3 task.py --experiment-name={EXPERIMENT_NAME} --num-workers=2 --logging-dir={RAY_JOB_LOGGING_URI}\",\n",
        "    runtime_env={\n",
        "        \"pip\": {\"packages\": str(ray_lab_local_dir / \"requirements.txt\")},\"working_dir\": str(script_path),\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zO-2Utrz1zy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709256428593,
          "user_tz": 360,
          "elapsed": 184,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "aa259fc6-ec0e-4d00-a484-891c4e380e6b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:ray.dashboard.modules.dashboard_sdk:Uploading package gcs://_ray_pkg_bf10ab91c4ff7dc2.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    job_status = ray_client.get_job_status(job_id)\n",
        "    if job_status == JobStatus.SUCCEEDED:\n",
        "        print(\"Job succeeded!\")\n",
        "        break\n",
        "    else:\n",
        "        if job_status == JobStatus.FAILED:\n",
        "            print(\"Job failed!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Job is running...\")\n",
        "            time.sleep(60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wquv8dasC_FL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709256488671,
          "user_tz": 360,
          "elapsed": 60081,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9426bbc0-0c36-4ff6-bffb-ac0e1051921d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job is running...\n",
            "Job succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [5] Model metrics in the job logs"
      ],
      "metadata": {
        "id": "YYBBwUcInHKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Navigate to the Ray on Vertex's Ray Dashboard.\n",
        "\n",
        "1.   Click on the jobs tab\n",
        "2.   Click on the job associated with your submission\n",
        "3.   Click on the logs link\n",
        "4.   Click on the driver log\n",
        "5.   You should see the model metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "KUsCL5iWnN1O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "XKc7Bc3w2vEe",
        "QMngEVZs2zt5",
        "0Ovp-WDT26mv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}