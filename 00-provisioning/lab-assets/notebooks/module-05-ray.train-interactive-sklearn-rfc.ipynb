{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Distributed Scikit-Learn with Ray on Vertex AI"
      ],
      "metadata": {
        "id": "QMngEVZs2zt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer,make_column_transformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix,confusion_matrix,classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform.preview import vertex_ray\n",
        "import ray\n",
        "from ray.runtime_env import RuntimeEnv\n",
        "from ray.air.config import RunConfig\n",
        "from ray.air import CheckpointConfig, ScalingConfig\n",
        "from ray.util.joblib import register_ray\n"
      ],
      "metadata": {
        "id": "XNGcXZjMHR3H",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709242855944,
          "user_tz": 360,
          "elapsed": 17538,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare, initialize\n",
        "project_id_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "PROJECT_ID = project_id_output[0]\n",
        "project_nbr_output = !gcloud projects describe $PROJECT_ID --format='value(projectNumber)'\n",
        "PROJECT_NBR = project_nbr_output[0]\n",
        "REGION=\"us-central1\"\n",
        "\n",
        "aiplatform.init(project='ray-of-sunshine', location='us-central1')\n",
        "RAY_ADDRESS=f\"vertex_ray://projects/{PROJECT_NBR}/locations/us-central1/persistentResources/ray-kicking-tires-cluster\""
      ],
      "metadata": {
        "id": "CQRpmJJBvop7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709242857946,
          "user_tz": 360,
          "elapsed": 2017,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, sys\n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "from ray.util.joblib import register_ray\n",
        "register_ray()\n",
        "\n",
        "RUNTIME_ENV = {\n",
        "  \"pip\": [\n",
        "      \"google-cloud-aiplatform[ray]==1.40.0\",\n",
        "      \"ray[data]==2.4.0\",\n",
        "      \"ray[train]==2.4.0\",\n",
        "      \"ray[tune]==2.4.0\",\n",
        "      \"scikit-learn==1.2.2\",\n",
        "      \"google-cloud-bigquery\",\n",
        "      \"google-cloud-aiplatform\",\n",
        "      \"joblib\",\n",
        "      \"pandas<2.0.0\"\n",
        "  ],\n",
        "}\n",
        "ray.shutdown()\n",
        "ray.init(address=RAY_ADDRESS,runtime_env=RUNTIME_ENV)\n",
        "\n",
        "# The below statement will parallelize all code placed below it\n",
        "with joblib.parallel_backend('ray'):\n",
        "\n",
        "  # Column listing\n",
        "  numerical_columns_list=[\"culmen_length_mm\",\"culmen_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]\n",
        "  categorical_columns_list=[\"island\", \"sex\"]\n",
        "\n",
        "  # Read training data from BigQuery\n",
        "  client = bigquery.Client()\n",
        "  source_df = client.query(\"SELECT * FROM `ray_lab_ds.penguins_curated`\").to_dataframe()\n",
        "\n",
        "  # Features\n",
        "  X = source_df.drop(columns = ['species'])\n",
        "\n",
        "  # Label\n",
        "  Y = source_df['species']\n",
        "\n",
        "  # Split into train and test data\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 123)\n",
        "\n",
        "  # Preprocessing of numerical data\n",
        "  numerical_transformer = SimpleImputer(strategy='mean')\n",
        "  numerical_scaler = MinMaxScaler()\n",
        "\n",
        "  # Preprocessing for categorical data\n",
        "  categorical_preprocessing_pipe = Pipeline(steps=[\n",
        "      ('cat_col_imputer', SimpleImputer(strategy='most_frequent')),\n",
        "      ('cat_col_onehotencoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "  ])\n",
        "\n",
        "  # Bundle preprocessing for numerical imputer and categorical preprocessing pipeline\n",
        "  preprocessor = ColumnTransformer(\n",
        "      transformers=[\n",
        "          ('num_col_imputer', numerical_transformer, numerical_columns_list),\n",
        "          ('cat_col_preprocessor', categorical_preprocessing_pipe, categorical_columns_list)\n",
        "      ])\n",
        "\n",
        "  random_forest_model = RandomForestClassifier(n_estimators=10)\n",
        "\n",
        "  # Bundle preprocessing and modeling code in a pipeline\n",
        "  penguin_training_pipeline = Pipeline(steps=[\n",
        "      ('preprocessor', preprocessor),\n",
        "      ('scaler', numerical_scaler),\n",
        "      ('model', random_forest_model)])\n",
        "\n",
        "  penguin_training_pipeline.fit(X_train, Y_train)\n",
        "\n",
        "  # Testing\n",
        "  penguin_predictions = penguin_training_pipeline.predict(X_test)\n",
        "  penguin_predictions\n",
        "\n",
        "  print('Accuracy : ', accuracy_score(Y_test, penguin_predictions))\n",
        "  print('F1 Score : ', f1_score(Y_test, penguin_predictions, average = 'weighted'))\n",
        "  print('Precision : ', precision_score(Y_test, penguin_predictions , average = 'weighted'))\n",
        "  print('Recall : ', recall_score(Y_test, penguin_predictions, average = 'weighted'))"
      ],
      "metadata": {
        "id": "EAunzkutuZH0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709243085803,
          "user_tz": 360,
          "elapsed": 41074,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07fc0cd2-dd21-4456-87a4-936e6afce744"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ray on Vertex AI]: Cluster State = State.RUNNING\n",
            "Accuracy :  1.0\n",
            "F1 Score :  1.0\n",
            "Precision :  1.0\n",
            "Recall :  1.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}